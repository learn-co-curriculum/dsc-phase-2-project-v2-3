{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# King County House Prices Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "In the world of real estate, homeowners often find themselves in a difficult dilemma: trying to understand the intricate web of factors that influence the price of their most cherished possessionâ€”their homes. In the world of real estate, homeowners often struggle to understand what makes their homes valuable. Imagine you're a homeowner. Your home is one of your most precious assets. But have you ever wondered why some homes are more expensive than others? It's a bit like a puzzle. Our project is all about solving this puzzle. We are embarking on a journey to empower homeowners with a deep understanding of the determinants of house prices through our project. This project is a collaborative effort with our stakeholder, a prominent real estate agency dedicated to guiding homeowners through the intricate process of buying and selling homes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "In the dynamic real estate market of King County, Washington, numerous households aspire to purchase homes. However, the ever-present information asymmetry often leaves these potential buyers navigating the market blindly. To address this prevalent challenge, our project undertakes an in-depth analysis of house sales data spanning the years 2014 to 2015 within King County. Our mission is to offer invaluable consultation services to a reputable real estate agency dedicated to assisting households in their pursuit of homeownership.\n",
    "\n",
    "Through a comprehensive examination of this dataset, we aim to bridge the information gap in the real estate market. Our objective is to provide a robust method for predicting house prices, enabling prospective buyers to make well-informed decisions about their property investments. In doing so, we empower both homebuyers and the real estate agency with the knowledge and insights needed to navigate the competitive King County housing landscape effectively.\n",
    "\n",
    "### Challenges\n",
    "- For homeowners, the challenge is understanding why their homes are worth a certain amount. This knowledge can help them decide if they should sell, renovate, or just enjoy their home as it is.\n",
    "- The real estate agency faces a challenge too, which is to give homeowners the best advice. To do that, they need to know what makes a home valuable and to provide precise advice, they need to understand what drives property prices.\n",
    "\n",
    "### Solutions\n",
    "- Our project's solution is rooted in the power of data analysis. We will embark on a comprehensive exploration of house prices and their underlying determinants. Our aim is to not only identify the fundamental factors that sway home prices but to quantify their influence. In doing so, we seek to provide homeowners with invaluable insights.\n",
    "- By uncovering the complexities of the property pricing landscape, we aim to enhance the services offered by the real estate agency. Our project is designed to be a beacon of clarity amid the maze of the real estate market.\n",
    "\n",
    "### Conclusion\n",
    "This project reflects our collective pursuit to empower homeowners with data-driven insights into their property's value and to unravel the intricate and compelling aspects of house pricing. It is our commitment to provide homeowners, buyers, and our partner agency with a crystal-clear perspective on the determinants of property value, culminating in a predictive model of unwavering accuracy.\n",
    "\n",
    "### Problem Statement\n",
    "In the dynamic real estate market of King County, Washington, prospective homebuyers often face challenges when attempting to understand the multifaceted factors that determine the prices of residential properties. This information gap can leave homeowners and potential buyers navigating the housing market without clear insights into the key determinants of property values. As a result, individuals struggle to make informed decisions when buying, selling, or investing in homes.\n",
    "\n",
    "### Objectives\n",
    "Our general objective is to investigate the effect of independent variables on the price of a house.\n",
    "\n",
    "The Specific Objectives are :\n",
    "1.  To Assess Correlations Between Independent Variables and House Prices.\n",
    "2.\tTo Investigate the Impact of Highly and Lowly Correlated Variables on House Prices.\n",
    "3.\tTo Develop a Robust Multilinear Regression Model for House Price Prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "We are going to start by importing all the necessary modules to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A separate section with all the functions will be created here as well. This was deemed as the best/most convenient approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_to_dataframe(file):\n",
    "    \"\"\"\n",
    "    Reads a file and returns its contents as a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - file (str): The path to the file to be read.\n",
    "\n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): The contents of the file as a DataFrame.\n",
    "\n",
    "    Raises:\n",
    "    - None\n",
    "\n",
    "    Examples:\n",
    "    >>> read_file_to_dataframe('data.csv')\n",
    "         col1   col2\n",
    "    0   value1  value2\n",
    "    1   value3  value4\n",
    "    \"\"\"\n",
    "    extension = file.split(\".\")[-1]\n",
    "    if extension == \"csv\":\n",
    "        df = pd.read_csv(file)\n",
    "    elif extension == \"xlsx\" or extension == \"xls\":\n",
    "        df = pd.read_excel(file)\n",
    "    elif extension == \"json\":\n",
    "        df = pd.read_json(file)\n",
    "    elif extension == \"parquet\":\n",
    "        df = pd.read_parquet(file)\n",
    "    elif extension == \"tsv\" or extension == \"txt\":\n",
    "        df = pd.read_csv(file, sep=\"\\t\")\n",
    "    elif extension == 'pkl':\n",
    "        df = pd.read_pickle(file)\n",
    "    else:\n",
    "        print(\"File format not supported\")\n",
    "        return\n",
    "    return df\n",
    "\n",
    "def dataframe_detailed(df):\n",
    "    \"\"\"\n",
    "    Print details of the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The dataframe to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(f\"DATAFRAME SHAPE: {df.shape}\\n\\n\")\n",
    "    print(f\"{df.info()}\\n\\n\")\n",
    "    print(f\"DATAFRAME HEAD:\\n {df.head()}\\n\\n\")\n",
    "    print(f\"DATAFRAME KEY STATISTIC DESCRIPTION:\\n {df.describe()}\\n\\n\")\n",
    "\n",
    "def map_replace_values(df, column, replacement_dict):\n",
    "    \"\"\"\n",
    "    Replace values in a DataFrame column using a dictionary mapping.\n",
    "\n",
    "    :param df: The DataFrame to modify.\n",
    "    :type df: pandas.DataFrame\n",
    "    :param column: The name of the column to replace values in.\n",
    "    :type column: str\n",
    "    :param replacement_dict: A dictionary mapping old values to new values.\n",
    "    :type replacement_dict: dict\n",
    "    :return: The modified DataFrame with replaced values.\n",
    "    :rtype: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    df[column] = df[column].map(replacement_dict)\n",
    "    return df\n",
    "\n",
    "def fit_simple_linear_regression(df, dependent_variable, independent_variable):\n",
    "    \"\"\"\n",
    "    Fits a simple linear regression model to the given dataframe.\n",
    "\n",
    "    :param df: The pandas dataframe containing the data.\n",
    "    :param dependent_variable: The name of the dependent variable column.\n",
    "    :param independent_variable: The name of the independent variable column.\n",
    "\n",
    "    :return: A tuple containing the root mean squared error (rmse),\n",
    "      the mean absolute error (mae), and the model summary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create and fit the simple linear regression model\n",
    "    y = df[dependent_variable]\n",
    "    X = df[independent_variable]\n",
    "    X = sm.add_constant(X)  # Add a constant for the intercept\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Calculate RMSE and MAE\n",
    "    predictions = model.predict(X)\n",
    "    residuals = y - predictions\n",
    "    rmse = np.sqrt(np.mean(residuals**2))\n",
    "    mae = np.mean(np.abs(residuals))\n",
    "\n",
    "    # Get the model summary\n",
    "    model_summary = model.summary()\n",
    "\n",
    "    return rmse, mae, model_summary\n",
    "\n",
    "def fit_linear_regression(df, dependent_variable, independent_variables):\n",
    "    \"\"\"\n",
    "    Fit a linear regression model to the given data.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input dataframe containing the data.\n",
    "        dependent_variable (str): The name of the dependent variable.\n",
    "        independent_variables (list): The list of independent variables.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the fitted model and its summary.\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        >>> df = pd.DataFrame({'x': [1, 2, 3], 'y': [2, 4, 6]})\n",
    "        >>> fit_linear_regression(df, 'y', ['x'])\n",
    "        (model, model_summary)\n",
    "    \"\"\"\n",
    "    if len(independent_variables) == 1:\n",
    "        # Simple Linear Regression\n",
    "        model_formula = f\"{dependent_variable} ~ {independent_variables[0]}\"\n",
    "        model = ols(model_formula, data=df).fit()\n",
    "    else:\n",
    "        # Multiple Linear Regression\n",
    "        X = df[independent_variables]\n",
    "        X = sm.add_constant(X)  # Add a constant for the intercept\n",
    "        y = df[dependent_variable]\n",
    "        model = sm.OLS(y, X).fit()\n",
    "\n",
    "    model_summary = model.summary()\n",
    "\n",
    "    return model, model_summary\n",
    "\n",
    "def model_metrics(df, dependent_variable, independent_variables):\n",
    "    \"\"\"\n",
    "    Calculate the root mean squared error (RMSE) and mean absolute error (MAE) for a linear regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The input DataFrame containing the data.\n",
    "    - dependent_variable (str): The name of the dependent variable column in the DataFrame.\n",
    "    - independent_variables (list): A list of names of the independent variable columns in the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - rmse (float): The root mean squared error.\n",
    "    - mae (float): The mean absolute error.\n",
    "    \"\"\"\n",
    "    # Create and fit the linear regression model\n",
    "    X = df[independent_variables]\n",
    "    X = sm.add_constant(X)  # Add a constant for the intercept\n",
    "    y = df[dependent_variable]\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Calculate RMSE\n",
    "    y_pred = model.predict(X)\n",
    "    rmse = (mean_squared_error(y, y_pred)) ** 0.5\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "\n",
    "    return rmse, mae\n",
    "\n",
    "def plot_residuals_ols_mlr(ols_model, mlr_model, X_ols, X_mlr, y):\n",
    "    \"\"\"\n",
    "    Generate plots of residuals for OLS and MLR models.\n",
    "\n",
    "    Parameters:\n",
    "    ols_model (object): The OLS model object.\n",
    "    mlr_model (object): The MLR model object.\n",
    "    X_ols (array-like): The input data for the OLS model.\n",
    "    X_mlr (array-like): The input data for the MLR model.\n",
    "    y (array-like): The target variable.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Calculate residuals for OLS and MLR models\n",
    "    residuals_ols = y - ols_model.predict(sm.add_constant(X_ols))\n",
    "    residuals_mlr = y - mlr_model.predict(sm.add_constant(X_mlr))\n",
    "\n",
    "    # Create a scatter plot of residuals vs. predicted values for OLS\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(ols_model.predict(sm.add_constant(X_ols)),\n",
    "                residuals_ols, alpha=0.5)\n",
    "    plt.title(\"OLS Residuals vs. Predicted Values\")\n",
    "    plt.xlabel(\"Predicted Values (OLS)\")\n",
    "    plt.ylabel(\"Residuals (OLS)\")\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "\n",
    "    # Create a scatter plot of residuals vs. predicted values for MLR\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(mlr_model.predict(sm.add_constant(X_mlr)),\n",
    "                residuals_mlr, alpha=0.5)\n",
    "    plt.title(\"MLR Residuals vs. Predicted Values\")\n",
    "    plt.xlabel(\"Predicted Values (MLR)\")\n",
    "    plt.ylabel(\"Residuals (MLR)\")\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "\n",
    "    # Create a histogram of residuals for OLS\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(residuals_ols, bins=30, alpha=0.75)\n",
    "    plt.title(\"Histogram of OLS Residuals\")\n",
    "    plt.xlabel(\"Residuals (OLS)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    # Create a histogram of residuals for MLR\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(residuals_mlr, bins=30, alpha=0.75)\n",
    "    plt.title(\"Histogram of MLR Residuals\")\n",
    "    plt.xlabel(\"Residuals (MLR)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_rmse_mae_multi(model, X, y):\n",
    "    \"\"\"\n",
    "    Calculate RMSE and MAE for a multiple linear regression model.\n",
    "\n",
    "    Args:\n",
    "    model: The fitted multiple linear regression model.\n",
    "    X: The feature matrix.\n",
    "    y: The true target values.\n",
    "\n",
    "    Returns:\n",
    "    rmse (float): Root Mean Squared Error.\n",
    "    mae (float): Mean Absolute Error.\n",
    "    \"\"\"\n",
    "    # Predict the target values using the model\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "\n",
    "    return rmse, mae\n",
    "\n",
    "def calculate_vif(df, target_variable):\n",
    "    \"\"\"\n",
    "    Calculate the Variance Inflation Factor (VIF) for each independent variable in a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input dataframe containing the independent variables.\n",
    "        target_variable (str): The name of the target variable.\n",
    "\n",
    "    Returns:\n",
    "        vif_data (DataFrame): A dataframe containing the variables and their corresponding VIF values.\n",
    "    \"\"\"\n",
    "    # Separate the target variable\n",
    "    independent_vars = df.drop(columns=[target_variable])\n",
    "\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Variable\"] = independent_vars.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(\n",
    "        independent_vars.values, i) for i in range(independent_vars.shape[1])]\n",
    "\n",
    "    # Handle cases with high VIF values (you can choose a threshold) by setting VIF to NaN\n",
    "    threshold = 5  # You can adjust this threshold based on your analysis\n",
    "    vif_data.loc[vif_data[\"VIF\"] > threshold, \"VIF\"] = np.nan\n",
    "\n",
    "    return vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "In this section, the identifcation, collection, and surface-level analysis of the data shall be done by:\n",
    "- Collecting initial data (Has been compiled into a csv file).\n",
    "- Describing the data we are working with.\n",
    "- Exploring the data for any relationships and trends.\n",
    "- Verifying the data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = read_file_to_dataframe('data/kc_house_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not examine the properties of the dataframe by calling the custom function `dataframe_detailed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATAFRAME SHAPE: (21597, 21)\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21597 entries, 0 to 21596\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   id             21597 non-null  int64  \n",
      " 1   date           21597 non-null  object \n",
      " 2   price          21597 non-null  float64\n",
      " 3   bedrooms       21597 non-null  int64  \n",
      " 4   bathrooms      21597 non-null  float64\n",
      " 5   sqft_living    21597 non-null  int64  \n",
      " 6   sqft_lot       21597 non-null  int64  \n",
      " 7   floors         21597 non-null  float64\n",
      " 8   waterfront     19221 non-null  object \n",
      " 9   view           21534 non-null  object \n",
      " 10  condition      21597 non-null  object \n",
      " 11  grade          21597 non-null  object \n",
      " 12  sqft_above     21597 non-null  int64  \n",
      " 13  sqft_basement  21597 non-null  object \n",
      " 14  yr_built       21597 non-null  int64  \n",
      " 15  yr_renovated   17755 non-null  float64\n",
      " 16  zipcode        21597 non-null  int64  \n",
      " 17  lat            21597 non-null  float64\n",
      " 18  long           21597 non-null  float64\n",
      " 19  sqft_living15  21597 non-null  int64  \n",
      " 20  sqft_lot15     21597 non-null  int64  \n",
      "dtypes: float64(6), int64(9), object(6)\n",
      "memory usage: 3.5+ MB\n",
      "None\n",
      "\n",
      "\n",
      "DATAFRAME HEAD:\n",
      "            id        date     price  bedrooms  bathrooms  sqft_living  \\\n",
      "0  7129300520  10/13/2014  221900.0         3       1.00         1180   \n",
      "1  6414100192   12/9/2014  538000.0         3       2.25         2570   \n",
      "2  5631500400   2/25/2015  180000.0         2       1.00          770   \n",
      "3  2487200875   12/9/2014  604000.0         4       3.00         1960   \n",
      "4  1954400510   2/18/2015  510000.0         3       2.00         1680   \n",
      "\n",
      "   sqft_lot  floors waterfront  view  ...          grade sqft_above  \\\n",
      "0      5650     1.0        NaN  NONE  ...      7 Average       1180   \n",
      "1      7242     2.0         NO  NONE  ...      7 Average       2170   \n",
      "2     10000     1.0         NO  NONE  ...  6 Low Average        770   \n",
      "3      5000     1.0         NO  NONE  ...      7 Average       1050   \n",
      "4      8080     1.0         NO  NONE  ...         8 Good       1680   \n",
      "\n",
      "   sqft_basement yr_built  yr_renovated  zipcode      lat     long  \\\n",
      "0            0.0     1955           0.0    98178  47.5112 -122.257   \n",
      "1          400.0     1951        1991.0    98125  47.7210 -122.319   \n",
      "2            0.0     1933           NaN    98028  47.7379 -122.233   \n",
      "3          910.0     1965           0.0    98136  47.5208 -122.393   \n",
      "4            0.0     1987           0.0    98074  47.6168 -122.045   \n",
      "\n",
      "   sqft_living15  sqft_lot15  \n",
      "0           1340        5650  \n",
      "1           1690        7639  \n",
      "2           2720        8062  \n",
      "3           1360        5000  \n",
      "4           1800        7503  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "\n",
      "DATAFRAME KEY STATISTIC DESCRIPTION:\n",
      "                  id         price      bedrooms     bathrooms   sqft_living  \\\n",
      "count  2.159700e+04  2.159700e+04  21597.000000  21597.000000  21597.000000   \n",
      "mean   4.580474e+09  5.402966e+05      3.373200      2.115826   2080.321850   \n",
      "std    2.876736e+09  3.673681e+05      0.926299      0.768984    918.106125   \n",
      "min    1.000102e+06  7.800000e+04      1.000000      0.500000    370.000000   \n",
      "25%    2.123049e+09  3.220000e+05      3.000000      1.750000   1430.000000   \n",
      "50%    3.904930e+09  4.500000e+05      3.000000      2.250000   1910.000000   \n",
      "75%    7.308900e+09  6.450000e+05      4.000000      2.500000   2550.000000   \n",
      "max    9.900000e+09  7.700000e+06     33.000000      8.000000  13540.000000   \n",
      "\n",
      "           sqft_lot        floors    sqft_above      yr_built  yr_renovated  \\\n",
      "count  2.159700e+04  21597.000000  21597.000000  21597.000000  17755.000000   \n",
      "mean   1.509941e+04      1.494096   1788.596842   1970.999676     83.636778   \n",
      "std    4.141264e+04      0.539683    827.759761     29.375234    399.946414   \n",
      "min    5.200000e+02      1.000000    370.000000   1900.000000      0.000000   \n",
      "25%    5.040000e+03      1.000000   1190.000000   1951.000000      0.000000   \n",
      "50%    7.618000e+03      1.500000   1560.000000   1975.000000      0.000000   \n",
      "75%    1.068500e+04      2.000000   2210.000000   1997.000000      0.000000   \n",
      "max    1.651359e+06      3.500000   9410.000000   2015.000000   2015.000000   \n",
      "\n",
      "            zipcode           lat          long  sqft_living15     sqft_lot15  \n",
      "count  21597.000000  21597.000000  21597.000000   21597.000000   21597.000000  \n",
      "mean   98077.951845     47.560093   -122.213982    1986.620318   12758.283512  \n",
      "std       53.513072      0.138552      0.140724     685.230472   27274.441950  \n",
      "min    98001.000000     47.155900   -122.519000     399.000000     651.000000  \n",
      "25%    98033.000000     47.471100   -122.328000    1490.000000    5100.000000  \n",
      "50%    98065.000000     47.571800   -122.231000    1840.000000    7620.000000  \n",
      "75%    98118.000000     47.678000   -122.125000    2360.000000   10083.000000  \n",
      "max    98199.000000     47.777600   -121.315000    6210.000000  871200.000000  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe_detailed(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when finding out more information about the dataframe, it was noted that not all columns have the same non-null values. Thus, we will establish exactly how many null values are in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                  0\n",
       "date                0\n",
       "price               0\n",
       "bedrooms            0\n",
       "bathrooms           0\n",
       "sqft_living         0\n",
       "sqft_lot            0\n",
       "floors              0\n",
       "waterfront       2376\n",
       "view               63\n",
       "condition           0\n",
       "grade               0\n",
       "sqft_above          0\n",
       "sqft_basement       0\n",
       "yr_built            0\n",
       "yr_renovated     3842\n",
       "zipcode             0\n",
       "lat                 0\n",
       "long                0\n",
       "sqft_living15       0\n",
       "sqft_lot15          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also take the time to establish whether there are any duplicate values in the dataframe. As there are multiple columns with numerical data, duplicates in those columns can be expected. However, when looking at unique columns e.g. the id, duplicates should ideally not be found.\n",
    "\n",
    "Despite this, duplicate IDs will be kept in the dataframe as the assumption is that these are people who are flipping houses, and the ID is not assigned to a particular transaction, but to a particular house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "The preparation of the final dataset is done by:\n",
    "- Removal of erroneous data.\n",
    "- Handling of duplicate data, and \n",
    "- Removal of null data.\n",
    "\n",
    "\n",
    "The process is initiated by dropping certain columns that are deemed unnecessary. These columns are: `date` and `yr_renovated`. This is not to say that other columns may not be dropped later, but this step simply eliminates all columns that are deemed surplus to requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
